\documentclass[12pt]{extarticle}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{extsizes}
\usepackage{float}
\usepackage{graphicx}
\usepackage[margin = 1in]{geometry}
\usepackage{hyperref}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\Cov}{\mathrm{Cov}}
\begin{document}
	
\title{Bayesian Data Analysis Assignment 1}
\author{Benjamin Cox, S1621312}
\date{\vspace{-5ex}}
\maketitle

\section*{Question 1}

\subsection*{a)}
Our probability vector is $\theta = (\theta_1, \dots, \theta_6)$ and our outcome vector is $c = (c_1, \dots, c_6).$ We are drawing from a multinomial distribution (in the same way that 10 Bern(p) trials are the same as 1 Bin(10,p) trial distributionally), ie $$c \sim \mathrm{Multinomial}(120, \theta).$$ Therefore the likelihood of $\theta$ given $c$ with $n$ trials is the following: $$L(\theta|c) = \frac{n!}{c_1!c_2!\cdots c_6!}\theta_1^{c_1}\cdots\theta_6^{c_6}.$$ A suitable conjugate prior for this would be the Dirichlet distribution (K is the number of possible outcomes, in our case 6), $$f(x| \alpha, K) = \frac{\Gamma(\sum_{i=1}^{K}\alpha_i)}{\prod_{i=1}^{k}\Gamma(\alpha_i)}\prod_{i=1}^{K}x_i^{\alpha_i-1}.$$
The Jeffrey's prior for the multinomial corresponds to a Dirichlet distribution with $$\alpha_i = 1/2\  \forall i\in \{1,\dots,K\}.$$

\subsection*{b)}
Our posterior distribution for $\theta$ is 
\begin{align*}
p(\theta|c) &\propto \left(\frac{\Gamma(3)}{\Gamma(0.5)^6}\theta_1^{-1/2}\cdots\theta_6^{-1/2}\right)\left(\frac{n!}{c_1!c_2!\cdots c_6!}\theta_1^{c_1}\cdots\theta_6^{c_6}\right)\\
&= \mathrm{Dirichlet}\left(\alpha = c+0.5, K = 6\right).
\end{align*}
The expected value of the Dirichlet Distribution is given by $\E\left[X_i\right] = \frac{\alpha_i}{\sum\alpha_i}$, so in our case $$\E\left[\theta_i|c\right] = \frac{c_i+\frac{1}{2}}{\sum c_i + 3}.$$

This corresponds to values of 
\begin{table}[H]
	\centering
	\begin{tabular}{rrrrrrr}
		\hline
		$\E\left[\theta_1\right]$ & $\E\left[\theta_2\right]$ & $\E\left[\theta_3\right]$ & $\E\left[\theta_4\right]$ & $\E\left[\theta_5\right]$ & $\E\left[\theta_6\right]$ \\ 
		\hline
		0.142 & 0.199 & 0.183 & 0.142 & 0.199 & 0.134 \\ 
		\hline
	\end{tabular}
\end{table}

We are going to compute symmetric 95\% credible intervals for each $\theta_i$, hence we must marginalise them. We could (theoretically) calculate a 95\% credible region in the 6 dimensional parameter space, but this would get extremely complicated really quickly, and would also be hard to interpret. 

Fortunately the marginal distributions of the Dirichlet are a lot easier, as they are beta distributions. Write $\alpha_0 = \sum\alpha_k,$ then we have 
$$\theta_i \sim \mathrm{Beta}\left(\alpha_i, \alpha_0 - \alpha_i\right).$$ We can substitute in our expressions for $\alpha_i$ to obtain $$\theta_i \sim \mathrm{Beta}\left(c_i + 1/2, c_0 - c_i + 2.5\right).$$ Using this result we obtain the following 95\% credible intervals:


\begin{table}[H]
	\centering
	\begin{tabular}{r|rr}
		\hline
		$\theta$ & Lower & Upper\\
		\hline
		$\theta_1$ & 0.08657456 &0.20897331\\
		$\theta_2$ &0.1337529 &0.2738778\\
		$\theta_3$ &0.1200040 &0.2556043\\
		$\theta_4$ &0.08657456 &0.20897331\\
		$\theta_5$ &0.1337529 &0.2738778\\
		$\theta_6$ &0.08007849 &0.19945622\\
	\end{tabular}
\end{table}

\subsection*{c)}
We are going to test the null hypothesis of a fair die. For the multinomial distribution there is no easy quantile function, so we are going to use the likelihood-ratio test and Pearson's $\chi^2$ test. These approach the true $p$-value from below and above respectively, so doing both should give us a very good idea.

Computing the $p$-value using the likelihood-ratio test we get $p_{lr} = 0.377$. Computing the $p$-value using Pearson's $\chi^2$ test we obtain $p_{\chi^2} = 0.377$. Thus we can be fairly sure that this is very close to the true $p$-value. Therefore we do not reject the null hypothesis that the die is fair.

\subsection*{d)}
The posterior predictive distribution is the `Dirichlet-Multinomial' distribution. The pmf for this is given by 
\[
f(x|n, \alpha) = \frac{n!\Gamma(\sum\alpha_i)}{\Gamma(n+\sum\alpha_i)}\prod_{k=1}^{K}\frac{\Gamma(x_k+\alpha_k)}{(x_k!)\Gamma(\alpha_k)}
\]
for $n$ the number of trials and $alpha_1, \dots, \alpha_k > 0.$ 

Taken as our posterior predictive under the Jeffrey's prior we have $$c_{\text{new}}\sim \mathrm{DirMNom(60, c + 1/2)}.$$
We can simulate from this. We draw 10,000 times from this distribution and find that with probability 0.737 we have more 5s than 6s in our next 60 trials.

\subsection*{e)}
We incorporate these into our likelihood, denoting the new count vector as $d$. Our new posterior is $$\theta \sim \mathrm{Dirichlet}(c+d+1/2, 6).$$ Our new posterior means are 
\begin{table}[H]
	\centering
	\begin{tabular}{rrrrrrr}
		\hline
		$\E\left[\theta_1\right]$ & $\E\left[\theta_2\right]$ & $\E\left[\theta_3\right]$ & $\E\left[\theta_4\right]$ & $\E\left[\theta_5\right]$ & $\E\left[\theta_6\right]$ \\ 
		\hline
		0.163 &0.183 &0.216 &0.138 &0.167 &0.138 \\ 
		\hline
	\end{tabular}
\end{table}
with 95\% marginal credible intervals given by 
\begin{table}[H]
	\centering
	\begin{tabular}{r|rr}
		\hline
		$\theta$ & Lower & Upper\\
		\hline
		$\theta_1$ &0.1189814 &0.2113785\\
		$\theta_2$ &0.1371556 &0.2340370\\
		$\theta_3$ &0.1667039 &0.2698204\\
		$\theta_4$ &0.0975285 &0.1838316\\
		$\theta_5$ &0.1225962 &0.2159303\\
		$\theta_6$ &0.0939960 &0.1791973\\
	\end{tabular}
\end{table}
The credible intervals have narrowed, as expected for more observations. It is of note that the new credible interval for $\theta_3$ (barely) does not contain the value required for a `fair' dice. This is evidence that the dice is not fair. 

\section*{Question 2}

\subsection*{a)}

We have two equally credible opinions on the distribution of $1/\lambda$. We have one stating that it lies mostly between 5 and 10, and another that states that it lies between 0 and 25. We are going to use the Gamma distribution as our prior for $\lambda$. 

We are going to calculate parameters for this such that the corresponding inverse gamma distributions has mean of the midpoint and standard deviation of 1/4 the range. 

Calculating these parameters we obtain 
\[
\alpha_1 = 38, \beta_1 = 277.5, \qquad \alpha_2 = 6, \beta_2 = 62.5,
\]
with the indices corresponding to the originating expert.

This means that our mixture prior is of the form 
\[
\lambda \sim 0.5\cdot\Gamma(38, 277.5) + 0.5\cdot\Gamma(6, 62.5)
\]

\subsection*{b)}

We have 100 points of data with a mean of 9.877 minutes. We calculate our posterior. The gamma distribution is a conjugate prior to the exponential distribution, so it is quite simple to calculate our new parameters. 

We calculate 
\[
\alpha_1^{post} = 138, \beta_1^{post} = 1265.2, \qquad \alpha_2^{post} = 106, \beta_2^{post} = 1050.2
\] using the formulae $\alpha^{post} = \alpha + n, \beta^{post} = \beta + \sum_i x_i = \beta + n\bar{x}.$ To calculate the posterior mixing proportion $Q$ we use \[Q = \frac{q\frac{\beta_1^{\alpha_1}}{\Gamma(\alpha_1)}\frac{\Gamma(\alpha_1^{post})}{\beta_1^{post \ \alpha_1^{post}}}}{q\frac{\beta_1^{\alpha_1}}{\Gamma(\alpha_1)}\frac{\Gamma(\alpha_1^{post})}{\beta_1^{post \ \alpha_1^{post}}} + (1-q)\frac{\beta_2^{\alpha_2}}{\Gamma(\alpha_2)}\frac{\Gamma(\alpha_2^{post})}{\beta_2^{post \ \alpha_2^{post}}}}.\]
This quantity involves extremely large numbers, so we work on the log scale and exponentiate at the end. 

Overall we obtain a posterior of the form
\[
p(\lambda|x) = 0.40455 \cdot \Gamma(138, 1265.2) + 0.59545 \cdot \Gamma(106, 1050.2). 
\]

Of course the posterior for $1/\lambda$ is analogous, with inverse gamma rather than gamma.

We calculate a posterior 95\% credible interval for $1/\lambda$ as $(8.00, 11.84)$, and a mean for $1/\lambda$ of 9.69.

To calculate the posterior probability of waiting for more than 20 minutes we need the posterior predictive distribution. We find that $$\Pr[x>20] = 0.127.$$ We note that the posterior distribution is a gamma mixture whereas the posterior predictive distribution is a generalised Pareto distribution. The pdf is given by 

\[
p(t_*|t) = 0.40455 \ f_l(\beta_1^{post}, \alpha_1^{post}) + 0.59545 \ f_l(\beta_2^{post}, \alpha_2^{post}),
\]

where 
\[
f_l(x|a,b) = \frac{a}{b}\left(1+\frac{x}{b}\right)^{-(a-1)}.
\]
This family of distributions is often used in survival analysis to model survival times, so it is natural that it could turn up here.

\subsection*{c)}
\begin{figure}[H]
	\centering
	\includegraphics[width = 0.5\textwidth]{../plpgoodnessplot}
	\caption{Prior-Likelihood-Posterior Plot}
	\label{fig:pgof}
\end{figure}

As seen in Figure (\ref{fig:pgof}) the posterior is highly influenced by the likelihood and not so much by the prior. The prior is compatible with the data as an initial estimate, and and served it's purpose well as the posterior has converged (mostly) onto the likelihood.

The prior is compatible as there is no conflict between the data and the posterior.

\subsection*{d)}
\begin{figure}[H]
	\centering
	\includegraphics[width = 0.5\textwidth]{../postpreddens}
	\caption{Posterior Predictive Density Plot, bright red line indicates mean, darker red lines indicate 95\% CI.}
	\label{fig:postpreddens}
\end{figure}

We plot our posterior predictive density. We obtain a mean of $9.69$ with symmetric 95\% credible interval given by $(0.24, \  36)$. This is very much in line with what we expect from the likelihood, indicating good fit.

\subsection*{e)}
We are now to take the Lindley distribution as our likelihood. This distribution has pdf of
\[
f(t|\lambda) = \frac{\lambda^2}{\lambda+1}(1+t)e^{-\lambda t}, \implies \ln(f(t|\lambda)) = 2\ln(\lambda) - \ln(\lambda + 1) + \ln(1+t) - \lambda t.
\]
The log pdf is important for implementing into our MC algorithm. 

Given our data we have likelihood of the form 
\[
L(\lambda|\underline{t}) = \left(\frac{\lambda^2}{\lambda+1}\right)^n\exp\left(-\lambda\sum_{i=1}^{n}t_i\right)\prod_{i=1}^{n}(1+t_i),
\]

hence we have a posterior of the form
\begin{align*}
p(\lambda|\underline{t}) &\propto \left(\Gamma(38, 277.5) + \Gamma(6, 62.5)\right)\left(\frac{\lambda^2}{\lambda+1}\right)^n\exp\left(-\lambda\sum_{i=1}^{n}t_i\right)\prod_{i=1}^{n}(1+t_i)\\
&\propto \left(\frac{277.5^{38}}{\Gamma(38)}\lambda^{37}e^{-277.5\cdot \lambda} + \frac{62.5^6}{\Gamma(6)}\lambda^{5}e^{-62.5\cdot \lambda}\right)\left(\frac{\lambda^2}{\lambda+1}\right)^n\exp\left(-\lambda\sum_{i=1}^{n}t_i\right)\prod_{i=1}^{n}(1+t_i)
\end{align*}
\end{document}

